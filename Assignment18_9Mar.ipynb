{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b779cc-624f-41da-a4b9-a611efcb5685",
   "metadata": {},
   "source": [
    "Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with\n",
    "an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709579e-3e93-4d2e-aeef-d7d6bd943f4e",
   "metadata": {},
   "source": [
    "### Probability Mass Function (PMF):\n",
    "#### Definition: The PMF is a function that gives the probability of a discrete random variable taking on a specific value. Denoted as P(X=x), where X is the random variable and x is a specific value.\n",
    "#### Properties:\n",
    "##### 1) 0≤P(X=x)≤1 for all possible values of x.\n",
    "##### 2) The sum of the probabilities over all possible values is equal to 1.\n",
    "#### Example: Consider a fair six-sided die. The PMF of the outcome of a single die roll is P(X=1)=P(X=2)=P(X=3)=P(X=4)=P(X=5)=P(X=6)= 1/6\n",
    "### Probability Density Function (PDF):\n",
    "#### Definition: The PDF is a function that gives the probability density of a continuous random variable at a specific point. Denoted as f(x), where x is a specific value.\n",
    "#### Properties: \n",
    "##### 1) f(x)≥0 for all x.\n",
    "##### 2) The area under the entire PDF curve is equal to 1.\n",
    "#### Example: Consider a standard normal distribution. The PDF of this distribution is the bell-shaped curve. \n",
    "### Comparision : \n",
    "#### PMF is associated with discrete random variables.\n",
    "#### PDF is associated with continuous random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2097c64-b53d-4585-a15b-0335570445d4",
   "metadata": {},
   "source": [
    "Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b95afd-9e07-4659-a98d-1bde522efaf3",
   "metadata": {},
   "source": [
    "#### The Cumulative Distribution Function (CDF) is a function that describes the probability that a random variable takes on a value less than or equal to a given point. It provides a cumulative measure of the distribution, incorporating all values up to a certain point.\n",
    "#### Definition: For a random variable X, the CDF, denoted as F(x), is defined as: F(x)=P(X≤x). In other words, the CDF at a specific point x is the probability that the random variable X is less than or equal to x.\n",
    "#### Properties:\n",
    "##### 1) Non-Decreasing: F(x) is a non-decreasing function. As x increases, F(x) can only stay the same or increase.\n",
    "##### 2) Limits: lim x tends to minus infinity (F(x))=0 and lim x tends to plus infinity (F(x))=1\n",
    "##### 3) Right-Continuous: F(x) is right-continuous, meaning there are no jumps in the function.\n",
    "#### Example: Consider a fair six-sided die. The CDF of the outcome of a single die roll is:\n",
    "##### F(x)=P(X≤x)\n",
    "##### For x≤1: F(x)=P(X≤1)= 1/6\n",
    "##### For 1<x≤2: F(x)=P(X≤2)= 2/6\n",
    "##### For 2<x≤3: F(x)=P(X≤3)= 3/6 \n",
    "##### For 3<x≤4: F(x)=P(X≤4)= 4/6\n",
    "##### For 4<x≤5: F(x)=P(X≤5)= 5/6\n",
    "##### For x>5: F(x)=P(X≤6)=6/6=1\n",
    "### Why CDF is Used?\n",
    "#### 1) Probability Calculations: CDF provides an easy way to calculate probabilities of random variables falling within specific intervals.\n",
    "#### 2) Quantiles and Percentiles: CDF is useful for determining quantiles (values corresponding to specific probabilities) and percentiles of a distribution.\n",
    "#### 3) Visualization: CDF plots provide a visual representation of the cumulative distribution, helping in understanding the overall shape and characteristics of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0642cbb-a687-4cfd-95bf-7c22d5ccda3d",
   "metadata": {},
   "source": [
    "Q3: What are some examples of situations where the normal distribution might be used as a model?\n",
    "Explain how the parameters of the normal distribution relate to the shape of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2264fb48-eb89-418f-80ff-9e590e5e5c9b",
   "metadata": {},
   "source": [
    "#### Here are some examples of situations where the normal distribution might be used as a model:\n",
    "#### i) Physical Measurements: Heights, weights, and other physical measurements of individuals in a population often follow a normal distribution.\n",
    "#### ii) Test Scores: In educational testing, scores on standardized tests, such as SAT or IQ tests, are often modeled using a normal distribution.\n",
    "#### iii) Financial Data: Stock prices, returns on investments, and other financial metrics often exhibit a normal distribution.\n",
    "#### iv) Errors in Measurements: Measurement errors in various scientific experiments are often assumed to follow a normal distribution.\n",
    "#### Parameters of the Normal Distribution:\n",
    "##### The normal distribution is characterized by two parameters: mean (μ) and standard deviation (σ). These parameters play a crucial role in determining the shape and characteristics of the distribution.\n",
    "#### 1) Mean (μ): The mean represents the central location of the distribution. It is the point around which the data is symmetrically distributed. Shifting the mean horizontally moves the entire distribution.\n",
    "#### 2) Standard Deviation (σ): The standard deviation measures the spread or dispersion of the distribution. A smaller standard deviation results in a narrower and taller curve. A larger standard deviation results in a wider and shorter curve.\n",
    "#### 3) Shape of the Distribution: The spread and symmetry of the normal distribution are controlled by the standard deviation. The mean determines the location of the peak of the curve. The curve is symmetric around the mean.\n",
    "#### 4) 68-95-99.7 Rule: In a normal distribution:\n",
    "##### Approximately 68% of the data falls within one standard deviation of the mean.\n",
    "##### Approximately 95% falls within two standard deviations.\n",
    "##### Approximately 99.7% falls within three standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e7f09d-5e63-4f53-9069-b5e773fdc4e6",
   "metadata": {},
   "source": [
    "Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal\n",
    "Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c9efd-a014-452e-892a-bcb0f4042555",
   "metadata": {},
   "source": [
    "#### The normal distribution is of great importance in statistics and probability theory due to its mathematical properties and its frequent occurrence in real-world phenomena. Some key reasons for the importance of the normal distribution include:\n",
    "#### 1) Central Limit Theorem: The normal distribution is a key component of the Central Limit Theorem, which states that the distribution of the sum (or average) of a large number of independent, identically distributed random variables approaches a normal distribution, regardless of the original distribution. This theorem is fundamental in inferential statistics.\n",
    "#### 2) Statistical Inference: Many statistical methods and hypothesis tests are based on the assumption of normality. For example, t-tests, analysis of variance (ANOVA), and linear regression often assume that the residuals are normally distributed.\n",
    "#### 3) Parameter Estimation: Maximum likelihood estimation and other statistical methods frequently assume normality for estimating parameters in various models.\n",
    "#### Real-Life Examples:\n",
    "#### 1) Height of Individuals: The distribution of heights in a population often follows a normal distribution, with the majority of individuals clustered around the mean height.\n",
    "#### 2) Exam Scores: Scores on standardized exams, such as SAT or GRE, are often assumed to be normally distributed.\n",
    "#### 3) Body Temperature: Human body temperatures, when measured accurately, are approximately normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a4d02d-40a7-4af6-9bee-2799f09fda2f",
   "metadata": {},
   "source": [
    "Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli\n",
    "Distribution and Binomial Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e181192-5d0b-4dd7-98e4-b7a8544146d9",
   "metadata": {},
   "source": [
    "### Bernoulli Distribution:\n",
    "#### The Bernoulli distribution is a discrete probability distribution that describes a random experiment with only two possible outcomes: success and failure. It is named after the Swiss mathematician Jacob Bernoulli. The distribution is characterized by a single parameter, often denoted as p, which represents the probability of success.\n",
    "### Example :\n",
    "#### Consider a coin toss, where we define \"heads\" as success (coded as 1) and \"tails\" as failure (coded as 0). If the probability of getting heads is p=0.6, then the Bernoulli distribution for this experiment would be:\n",
    "#### P(X=k)=0.6 if k=1\n",
    "#### P(X=k)=0.4 if k=\n",
    "### Difference between Bernoulli and Binomial Distributions:\n",
    "### 1) Number of Trials:\n",
    "#### Bernoulli Distribution: Describes a single trial or experiment with two possible outcomes.\n",
    "#### Binomial Distribution: Describes the number of successes in a fixed number of independent and identically distributed Bernoulli trials.\n",
    "### 2) Parameters:\n",
    "#### Bernoulli Distribution: Characterized by a single parameter p, which is the probability of success.\n",
    "#### Binomial Distribution: Characterized by two parameters n (number of trials) and p (probability of success in each trial).\n",
    "### 3) Outcome:\n",
    "#### Bernoulli Distribution: Deals with a single binary outcome (success or failure).\n",
    "#### Binomial Distribution: Deals with the number of successes in a fixed number of independent trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21e9f52-f5c4-408c-a877-7ff19b8cc460",
   "metadata": {},
   "source": [
    "Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset\n",
    "is normally distributed, what is the probability that a randomly selected observation will be greater\n",
    "than 60? Use the appropriate formula and show your calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d3dd5-e982-495c-8194-b6a217ea86dc",
   "metadata": {},
   "source": [
    "#### The z-score is calculated using the formula:\n",
    "#### z= (X-μ)/σ \n",
    "#### where: X is the value we're interested in (60 in this case) , μ is the mean of the dataset, σ is the standard deviation of the dataset.\n",
    "#### Given that the mean is 50, the standard deviation is 10, and we want to find the probability for X>60, we substitute these values into the formula: z=(60-50)/10=1\n",
    "#### Now, we look up the probability associated with a z-score of 1 in the standard normal distribution table. The probability that a randomly selected observation is greater than 60 can be found by subtracting the cumulative probability from the mean to the z-score:\n",
    "#### P(X>60)=1−P(Z≤1)=1−0.8413=0.1587\n",
    "#### So, the probability that a randomly selected observation from the dataset is greater than 60 is approximately 0.1587, or 15.87%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a2442f-3550-4365-972f-6d62d82cb341",
   "metadata": {},
   "source": [
    "Q7: Explain uniform Distribution with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251604db-3e42-4c81-a4df-cb0cc5e8d206",
   "metadata": {},
   "source": [
    "#### The uniform distribution is a continuous probability distribution that describes a random variable with equal probability of taking any value within a specified range. In other words, all outcomes within the range are equally likely. The probability density function (PDF) of a continuous uniform distribution is flat and constant over the entire range.\n",
    "#### The probability density function of a uniform distribution on the interval [a,b] is given by:\n",
    "#### f(x)= 1/(b-a)  for a<=x<=b \n",
    "#### Where: f(x) is the probability density function, a and b are the lower and upper bounds of the distribution.\n",
    "#### In the uniform distribution, the probability is evenly spread across the range, making it a simple and intuitive model for situations where each outcome within a given interval is equally likely.\n",
    "#### Example: Consider a six-sided fair die. The outcomes (1, 2, 3, 4, 5, 6) are equally likely when the die is rolled. The probability of getting any particular face is 1/6. This situation can be modeled by a discrete uniform distribution.\n",
    "#### Now, in a continuous uniform distribution representing the time it takes for an event to occur within a 10-minute window. If the event is equally likely to occur at any moment within that 10-minute interval, we can use a continuous uniform distribution with a=0 and b=10 minutes. The probability density function would be f(x)= 1/10 for 0<=x<=10 . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f9ad2-e856-4fca-9fa4-00506548014b",
   "metadata": {},
   "source": [
    "Q8: What is the z score? State the importance of the z score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4900e4-da70-4fc9-9ff1-a2d0cb2e0600",
   "metadata": {},
   "source": [
    "#### The z-score, also known as the standard score or z-value, is a statistical measure that describes a data point's relation to the mean of a group of data points. It is expressed in terms of standard deviations from the mean. The formula for calculating the z-score of a data point X in a dataset with mean μ and standard deviation σ is given by: \n",
    "#### z=(X-μ)/σ \n",
    "#### Here: X is the individual data point, μ is the mean of the dataset, σ is the standard deviation of the dataset.\n",
    "#### The z-score indicates how many standard deviations a data point is from the mean. A positive z-score means the data point is above the mean, while a negative z-score means the data point is below the mean.\n",
    "#### Importance of the Z-Score:\n",
    "#### 1) Standardization and Comparison: Z-scores standardize data, allowing for the comparison of scores from different distributions. It helps in assessing how unusual or typical a particular data point is within a dataset.\n",
    "#### 2) Identification of Outliers: Z-scores are useful for identifying outliers. Data points with z-scores significantly higher or lower than the mean may be considered outliers.\n",
    "#### 3) Normal Distribution Analysis: In a standard normal distribution (a normal distribution with a mean of 0 and standard deviation of 1), z-scores directly correspond to percentiles.  A z-score of 1.96, for example, corresponds to the 97.5th percentile.\n",
    "#### 4) Probability Calculations: Z-scores are used in probability calculations for normal distributions. The z-table or calculator provides probabilities associated with different z-scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2160390-56c6-4121-aa95-d350827c5a3f",
   "metadata": {},
   "source": [
    "Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402925f3-6215-4c3f-a98d-6b6731431b43",
   "metadata": {},
   "source": [
    "### Central Limit Theorem (CLT):\n",
    "#### The Central Limit Theorem is a fundamental concept in statistics that describes the distribution of sample means (or other sample statistics) drawn from any population, regardless of the population's underlying distribution. In essence, the theorem states that as the sample size increases, the distribution of the sample means approaches a normal distribution, even if the original population distribution is not normal.\n",
    "#### The Central Limit Theorem is often stated in the following way:\n",
    "#### If X1, X2, ..., Xn are independent and identically distributed random variables with a mean μ and standard deviation σ, and n is sufficiently large, then the distribution of the sample mean will be approximately normal with a mean μ and a standard deviation σ/sqrt(n). \n",
    "### Significance of the Central Limit Theorem:\n",
    "#### 1) Normal Approximation: The Central Limit Theorem allows us to use the normal distribution as an approximation for the distribution of sample means, even when the original population distribution is not normal. This is crucial for making statistical inferences.\n",
    "#### 2) Inference for Large Samples: For large sample sizes, statistical tests and confidence intervals that are based on the normal distribution can be applied, regardless of the population distribution. This simplifies statistical analysis.\n",
    "#### 3) Foundation for Hypothesis Testing: Many statistical tests and procedures, such as t-tests and confidence intervals, rely on the assumption of normality. The CLT provides a justification for using these procedures in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537ef9e0-ccb9-4b73-ab6d-a64bb8fe4ab2",
   "metadata": {},
   "source": [
    "Q10: State the assumptions of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f86a55a-6d3c-4c0c-98f5-6ca08f329492",
   "metadata": {},
   "source": [
    "#### The Central Limit Theorem (CLT) is a powerful statistical concept, but it relies on certain assumptions to be valid. These assumptions are crucial for the theorem to apply accurately. Here are the key assumptions of the Central Limit Theorem:\n",
    "#### 1) Independence: The random variables in the sample must be independent. This means that the occurrence or value of one observation should not influence the occurrence or value of another observation.\n",
    "#### 2) Identically Distributed: The random variables in the sample should be identically distributed. This implies that each observation is drawn from the same probability distribution.\n",
    "#### 3) Finite Mean and Standard Deviation: The population from which the samples are drawn should have a finite mean (μ) and a finite standard deviation (σ). If the mean and standard deviation are infinite, the CLT may not hold.\n",
    "#### 4) Sample Size is \"Sufficiently Large\": The Central Limit Theorem assumes that the sample size is \"sufficiently large.\" There is no fixed threshold for what constitutes a \"sufficiently large\" sample size, but as a rule of thumb, a sample size of 30 or greater is often considered adequate. However, the larger the sample size, the more robust the approximation to normality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
